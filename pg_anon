#! /usr/bin/env python

import os
import json
import boto3
import click
import random

from sys import exit
from sqlalchemy import create_engine
from typing import Any
import sys
from datetime import datetime

current_dir = os.path.dirname(os.path.abspath(__file__))
src_path = os.path.join(current_dir, "src")
extend_path = os.getenv("PGANON_EXTEND_DIR", os.path.join(current_dir, "extend"))
sys.path.append(src_path)
sys.path.append(extend_path)

# being a little lazy here and don't want to explicitly pass this into every function and class
if os.getenv("PGANON_LOG_DIR"):
    log_dir = os.getenv("PGANON_LOG_DIR")
else:
    log_dir = os.path.join(current_dir, "logs")

if not os.getenv("PGANON_ENVIRONMENT"):
    print("ERROR: PGANON_ENVIRONMENT must be set")
    exit(1)

os.environ["PGANON_LOG_PATH"] = log_dir
log_filename = f"{os.getenv("PGANON_ENVIRONMENT")}-{datetime.now():%Y-%m-%d_%H-%M}.log"
os.environ["PGANON_LOG_FILE"] = log_filename

from src.pganon.anonymizer import Anonymizer  # noqa: E402
from src.pganon.loglib import setup_logging, logger  # noqa: E402
from src.pganon.aws_utils import AWSUtils  # noqa: E402
from src.pganon.pg_rds_utils import PgRdsUtils  # noqa: E402
from src.pganon.state_utils import StateUtils  # noqa: E402
from src.pganon.serializer import default_serializer  # noqa: E402

aws_utils = AWSUtils()

def random_pass(pass_length=32):
    """Generate a random password."""
    character_set = (
        "abcdefghijklmnopqrstuvwxyz01234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ"
    )
    password = "".join(random.sample(character_set, pass_length))
    return password

@click.command()
@click.option(
    "--create-admin-password",
    type=str,
    flag_value="",  # Empty string when used as flag without value
    default=None,
    help="Update the password of the existing database user. If a password is provided, it will be used; otherwise a random password will be generated.",
)
@click.option(
    "--download",
    is_flag=True,
    help="Download the file from S3 to the expected output name without processing.",
)
@click.option(
    "--dry-run",
    is_flag=True,
    help="Don't actually modify the database instance, just output the state changes.",
)
@click.option(
    "--initialize", is_flag=True, help="Initialize the file if it does not exist."
)
@click.option(
    "--local-state",
    is_flag=True,
    help="Use the local state file instead of the remote state file.",
)
@click.option(
    "--overwrite", is_flag=True, help="Overwrite the output file if it already exists."
)
@click.option(
    "--save", is_flag=True, help="Save the output file locally after processing."
)
@click.option(
    "--savedb", is_flag=True, help="Save and re-use the database information."
)

@click.option(
    "--snapshot",
    is_flag=True,
    help="Save a snapshot as pganon-[value of PGANON_ENVIRONMENT] and delete the instance. Snapshot will be overwritten by each run.",
)
@click.option(
    "--upload",
    is_flag=True,
    help="Upload the existing state file to S3 without processing.",
)
@click.option(
    "--test-secret",
    is_flag=True,
    help="Test writing/updating a secret in AWS Secrets Manager with dummy data and exit.",
)

@click.option(
    "--test-errors",
    is_flag=True,
    help="Trigger a test of sns error publishing.",
)

@click.option("--verbose", is_flag=True, help="print verbose output")
@click.option(
    "--write-secret", type=str, help="Create a secret in AWS Secrets Manager."
)
def main(**kwargs) -> None:
    setup_logging()
    create_admin_password = kwargs.get("create_admin_password")
    download = kwargs.get("download")
    dry_run = kwargs.get("dry_run")
    initialize = kwargs.get("initialize")
    local_state = kwargs.get("local_state")
    overwrite = kwargs.get("overwrite")
    save = kwargs.get("save")
    savedb = kwargs.get("savedb")
    snapshot = kwargs.get("snapshot")
    upload = kwargs.get("upload")
    test_secret = kwargs.get("test_secret")
    verbose = kwargs.get("verbose")
    write_secret = kwargs.get("write_secret")
    test_errors = kwargs.get("test_errors")

    if not test_errors:
        if os.environ.get("PGANON_TEST_ERRORS"):
            test_errors = True

    if test_errors:
        logger.info("test_errors is enabled...")

    if dry_run and snapshot:
        logger.error("--snapshot and --dry-run cannot be used together since they can create a snapshot with original data!")
        exit(1)

    # Retrieve environment variables using standard psql names
    db_user = os.getenv("PGUSER")
    db_password = os.getenv("PGPASSWORD")
    db_port = os.getenv("PGPORT", "5432")
    db_name = os.getenv("PGDATABASE", "postgres")
    rds_source_id = os.getenv("PGANON_RDS_SOURCE_ID")
    s3_bucket_name = os.getenv("PGANON_S3_BUCKET_NAME")
    pganon_environment = os.getenv("PGANON_ENVIRONMENT")

    # this should trigger if:
    # 1) --create-admin-password is not used but PGANON_CREATE_ADMIN_PASSWORD is
    # 2) --create-admin-password is used with no argument (empty string value)
    if not create_admin_password:
        if os.getenv("PGANON_CREATE_ADMIN_PASSWORD"):
            # if PGANON_CREATE_ADMIN_PASSWORD is true, create a random password
            if os.getenv("PGANON_CREATE_ADMIN_PASSWORD") == "true":
                create_admin_password = random_pass()
            else:
                # if PGANON_CREATE_ADMIN_PASSWORD is set to a value, use that value
                create_admin_password = os.getenv("PGANON_CREATE_ADMIN_PASSWORD")

    if not all(
        [
            db_user,
            db_password,
            db_port,
            db_name,
            rds_source_id,
            s3_bucket_name,
            pganon_environment,
        ]
    ):
        logger.error(
            "One or more required environment variables (PGUSER, PGPASSWORD, PGPORT, PGDATABASE, PGANON_RDS_SOURCE_ID, PGANON_S3_BUCKET_NAME, PGANON_ENVIRONMENT) are not set."
        )
        exit(1)

    logger.info(f"Setting extend path to {extend_path}")
    pganon_data_dir = os.getenv("PGANON_DATA_DIR", "./data")
    logger.info(f"Setting data dir to {pganon_data_dir}")
    db_timeout = int(os.getenv("PGANON_DB_TIMEOUT", 30))
    results_file_name = f"{pganon_data_dir}/results_{rds_source_id}_{pganon_environment}.json"

    # sys.exit(0)

    # Check for the environment variable 'SAVE_DB'
    env_savedb = os.getenv("PGANON_SAVE_DB", "false").lower() == "true"
    savedb = savedb or env_savedb
    if savedb:
        logger.info("Saving database connection information to local file.")

    env_local_state = os.getenv("PGANON_LOCAL_STATE", "false").lower() == "true"
    local_state = local_state or env_local_state
    if local_state:
        logger.info("Using local state file instead of remote state file.")

    # check if snapshot is set and PGANON_ENVIRONMENT is set for snapshot naming
    if snapshot and not os.getenv("PGANON_ENVIRONMENT"):
        logger.error("PGANON_ENVIRONMENT must be set to create a snapshot")
        exit(1)

    target_account_id = os.getenv("PGANON_TARGET_ACCOUNT_ID")

    snapshot_id = f"pganon-{os.getenv('PGANON_ENVIRONMENT')}"

    # don't delete the local state file local-state is set
    if local_state:
        save_remote_state = True

    # get the source and target regions
    session = boto3.Session()
    source_region_name = os.getenv("PGANON_SOURCE_AWS_REGION", None)
    if not source_region_name:
        try:
            source_region_name = session.region_name
        except Exception as e:
            logger.error(
                f"No AWS region found, Please configure the PGANON_SOURCE_AWS_REGION environment variable. Error: {e}"
            )
            exit(1)

    # determine target AWS region (env var takes priority, otherwise inherit the session's region)
    target_region_name = os.getenv("PGANON_TARGET_AWS_REGION", None)
    if not target_region_name:
        target_region_name = session.region_name

    # -----------------------------------------------------------------
    # Optional quick-exit path: just test secret creation/update
    # -----------------------------------------------------------------
    if test_secret:
        dummy_secret_name = os.getenv("PGANON_TEST_SECRET_NAME", "/infra/secrets-test/rds/pg-anon/credentials")
        dummy_secret_body = {"test": "secret_update"}
        aws_utils.save_secret_to_aws(
            dummy_secret_name,
            dummy_secret_body,
            target_account_id=target_account_id,
            target_region=target_region_name,
        )
        logger.info(f"Test secret update completed for {dummy_secret_name}")
        exit(0)

    if test_errors:
        boto3.client('sns').publish(TopicArn=os.environ['SNS_TOPIC_ARN'],
                    Message='error testing',
                    Subject='pg-anon batch failed')
        logger.info("Test script failure.")
        exit(1)

    # some operations don't require database connection
    connect_db = True
    if upload or download:
        logger.info("Database connection not required for upload/download operations.")
        connect_db = False

    try:
        source_host = aws_utils.get_rds_host_by_instance_id(
            rds_source_id, source_region_name
        )
    except Exception as e:
        logger.error(f"Failed to get the host of the source instance: {e}")
        exit(1)

    state_utils = StateUtils()
    defaults = state_utils.read_defaults(source_host=source_host)

    if connect_db:
        if not os.getenv("PGANON_ENVIRONMENT"):
            logger.info("Environment variable PGANON_ENVIRONMENT not set.")
            exit(1)

        if os.getenv("PGANON_CROSS_ACCOUNT_ROLE_ARN") or os.getenv("PGANON_CROSS_ACCOUNT_EXTERNAL_ID"):
            write_secret = True
            logger.info("Auto-enabling write_secret due to environment variables set: PGANON_CROSS_ACCOUNT_ROLE_ARN, PGANON_CROSS_ACCOUNT_EXTERNAL_ID")

        if target_account_id:
            if not os.getenv("PGANON_CROSS_ACCOUNT_ROLE_ARN") or not os.getenv("PGANON_CROSS_ACCOUNT_EXTERNAL_ID"):
                logger.warning(f"External Account {target_account_id} referenced, but PGANON_CROSS_ACCOUNT_ROLE_ARN and/or PGANON_CROSS_ACCOUNT_EXTERNAL_ID not set to write remote secret.")

        aws_secret_name = None
        if write_secret:
            # set a default value for the secret name
            aws_secret_name = f"/infra/{os.getenv('PGANON_ENVIRONMENT')}/rds/pg-anon/credentials"
            logger.info(f"Using default secret name: {aws_secret_name}")

            # if the user has set a specific secret name, use that
            if os.getenv("PGANON_CREDS_SECRET"):
                aws_secret_name = os.getenv("PGANON_CREDS_SECRET")
                logger.info(f"Using custom secret name: {aws_secret_name}")
        else:
            # if write_secret is False, set aws_secret_name to None
            logger.info("write_secret is False, aws_secret_name set to None")

        logger.info(f"aws_secret_name set to: {aws_secret_name}")

        # get the host of the replica
        logger.info(f"processing host: {source_host}:{db_port}/{db_name}")

        src_connection_string = f"postgresql://{db_user}:{db_password}@{source_host}:{db_port}/{db_name}"
        logger.debug(f"connection_string: {src_connection_string}")
        source_engine_init = create_engine(
            src_connection_string,
            connect_args={"connect_timeout": db_timeout},
        )
        source_engine = PgRdsUtils(source_engine_init, source_host, defaults)

    # Set the output file name to match the original source host
    output_file_base = f"inspectstate_{source_host}.json"
    output_file = f"{pganon_data_dir}/{output_file_base}"

    # save remote state to local
    save_remote_state = False

    logger.debug(f"overwrite: {overwrite}")
    if os.path.exists(output_file) and not (
        overwrite or download or upload or local_state
    ):
        logger.error(
            f"Output file {output_file} already exists. Use --overwrite to overwrite it."
        )
        exit(1)

    # initialize the file if it does not exist
    elif initialize:
        data = source_engine.inspect_db(initialize)
        with open(output_file, "w") as f:
            json.dump(data, f, indent=4, default=default_serializer)
        logger.info(f"Initialized {output_file}.")
        save_remote_state = True

    # If download is set, download the file from S3 and exit
    elif download:
        save_remote_state = True
        if aws_utils.download_from_s3(s3_bucket_name, output_file_base, output_file):
            logger.info(f"File {output_file_base} downloaded from S3.")
            exit(0)
        else:
            logger.error(f"Failed to download {output_file_base} from S3.")
            exit(1)

    # If upload is set, upload the existing file to S3 and exit
    elif upload:
        if (
            aws_utils.download_from_s3(
                s3_bucket_name, output_file_base, f"{pganon_data_dir}/test_{output_file_base}"
            )
            and not overwrite
        ):
            logger.error(
                f"{output_file} already exists, use --overwrite to overwrite it."
            )
            os.remove(f"{pganon_data_dir}/test_{output_file_base}")
            exit(1)
        if overwrite and os.path.exists(f"{pganon_data_dir}/test_{output_file_base}"):
            os.remove(f"{pganon_data_dir}/test_{output_file_base}")
        if os.path.exists(output_file):
            aws_utils.upload_to_s3(s3_bucket_name, output_file_base, pganon_data_dir)
            logger.info(f"Existing file {output_file_base} uploaded to S3.")
            state_utils.save_or_remove(output_file, save_remote_state)
            exit(0)
        else:
            logger.error(
                f"Output file {output_file} does not exist locally. Cannot upload to S3."
            )
            exit(1)

    # If check is set, compare the inspectstate with the current database state
    else:
        # get the stored state from the file or s3
        stored_state = state_utils.read_existing_state(output_file_base, s3_bucket_name, output_file, local_state)
        # create a replica of the instance
        if savedb:
            db_save_file = f"{pganon_data_dir}/db_save-{pganon_environment}.json"
            if os.path.exists(db_save_file):
                logger.info(
                    f"Using cached database connection information from {db_save_file}."
                )
                tmp_rds_instance = json.load(open(db_save_file))
            else:
                logger.info(f"Saving database connection information to {db_save_file}.")
                tmp_rds_instance = aws_utils.create_instance_from_latest_snapshot(
                    rds_source_id, target_region_name, db_user, db_password
                )
            with open(db_save_file, "w") as f:
                json.dump(tmp_rds_instance, f, indent=4, default=default_serializer)
        else:
            tmp_rds_instance = aws_utils.create_instance_from_latest_snapshot(
                rds_source_id, target_region_name, db_user, db_password
            )
        logger.info(
            f"Creating target engine with endpoint: {tmp_rds_instance['endpoint']}"
        )
        target_engine_init = create_engine(
            f'postgresql://{db_user}:{db_password}@{tmp_rds_instance["endpoint"]}:{db_port}/{db_name}'
        )
        target_engine = PgRdsUtils(target_engine_init, source_host, defaults)
        state_utils = StateUtils(target_engine)

        if state_utils.match_state(
            stored_state, output_file, target_engine, initialize, save
        ):
            logger.info("Database state matches the existing file.")
            target_engine.execute_patch_sql(source_host, stage="pre")
            anonymizer = Anonymizer(
                target_engine_init, source_host, defaults, dry_run, verbose
            )
            target_state_data = state_utils.state_to_dict(output_file)
            result = anonymizer.anonymize_data(target_state_data)
            new_db_owner = {}
            if result:
                tmp_rds_instance["database"] = db_name
                tmp_rds_instance["date"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                logger.info("Anonymization completed successfully.")
                target_engine.execute_patch_sql(source_host, stage="post")
                if create_admin_password is not None:
                    # If create_admin_password is empty string (flag without value), pass None to generate random password
                    # If it has a value, use that password
                    password_to_use = create_admin_password if create_admin_password else None
                    new_db_owner = target_engine.create_new_db_owner(
                        user_name=None, db_passwd=password_to_use
                    )
                    logger.info(f"Updated admin user password: {new_db_owner}")
                if new_db_owner:
                    tmp_rds_instance["username"] = new_db_owner["db_user"]
                    tmp_rds_instance["password"] = new_db_owner["db_passwd"]
                final_log_message = f"Instance information: {tmp_rds_instance}"
                if snapshot:
                    aws_utils.delete_snapshot_if_exists(snapshot_id)
                    snapshot_info = aws_utils.delete_instance(
                        tmp_rds_instance["id"], snapshot_id
                    )
                    logger.info(
                        f"Snapshot {snapshot_id} created and instance {tmp_rds_instance['id']} deleted."
                    )
                    # Share the snapshot with target account if specified
                    if target_account_id:
                        aws_utils.share_snapshot_with_account(snapshot_id, target_account_id)
                    del tmp_rds_instance["port"]
                    del tmp_rds_instance["endpoint"]
                    tmp_rds_instance["source_instance_id"] = tmp_rds_instance["id"]
                    del tmp_rds_instance["id"]
                    tmp_rds_instance["snapshot_id"] = snapshot_id
                    tmp_rds_instance["snapshot_arn"] = snapshot_info["snapshot_arn"]
                    os.remove(db_save_file)
                    final_log_message = f"Snapshot information: {tmp_rds_instance}"
                with open(results_file_name, "w") as results_file:
                    json.dump(
                        tmp_rds_instance,
                        results_file,
                        indent=4,
                        default=default_serializer,
                    )
                logger.info(f"Results written to {results_file_name}.")
                logger.info("pg_anon completed successfully!")
                logger.info(final_log_message)

                if aws_secret_name:
                    aws_utils.save_secret_to_aws(
                        aws_secret_name,
                        tmp_rds_instance,
                        target_account_id=target_account_id,
                        target_region=target_region_name,
                    )
            else:
                logger.error("Anonymization failed.")
                exit(1)
        else:
            logger.error(
                "Database state does not match the existing file, humans, please review this!"
            )
            exit(1)
    state_utils.save_or_remove(output_file, save_remote_state)


if __name__ == "__main__":
    main()
